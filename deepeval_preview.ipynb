{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f971f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1-Distill-Qwen-32B\n",
      "DeepSeek-R1-Distill-Qwen-14B\n",
      "deepseek/deepseek-r1\n",
      "openai/gpt-4.1\n",
      "openai/o3\n",
      "anthropic/claude-sonnet-4\n",
      "google/gemini-2.5-pro-preview\n",
      "x-ai/grok-3-beta\n",
      "共有13个领域，每个领域有10个问题\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from deepeval import evaluate\n",
    "from deepeval.evaluate import DisplayConfig\n",
    "display_config = DisplayConfig(\n",
    "    show_indicator=True,\n",
    "    print_results=False,\n",
    "    verbose_mode=False,\n",
    ")\n",
    "import toml\n",
    "with open('config.toml', 'r', encoding='utf-8') as toml_file:\n",
    "    config = toml.load(toml_file)\n",
    "\n",
    "model_names = config['model_names']\n",
    "alter_names = model_names.copy() # Create a copy to modify\n",
    "\n",
    "for i, name in enumerate(alter_names):\n",
    "    if name in config['alternative_names']: \n",
    "        alter_names[i] = config['alternative_names'][name]\n",
    "\n",
    "for name in alter_names:\n",
    "    print(name)\n",
    "INPUT_EXCEL_FILE = \"goldens\" / Path(config['QA_file_name'])\n",
    "quesion_dfs = pd.read_excel(INPUT_EXCEL_FILE, sheet_name=None, index_col=0)\n",
    "DOMAIN = list(quesion_dfs.keys())\n",
    "num_questions_per_domain = quesion_dfs[DOMAIN[0]].shape[0]\n",
    "print(f\"共有{len(DOMAIN)}个领域，每个领域有{num_questions_per_domain}个问题\")\n",
    "\n",
    "QA_df = {}\n",
    "for i, model in enumerate(model_names):\n",
    "    QA_FILE = \"QA\" / Path(f\"{model}_answers.xlsx\")\n",
    "    QA_df[model_names[i]] = pd.read_excel(QA_FILE, sheet_name=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30a0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import get_dataset, correctness_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ab34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "test_modelA = model_names[0]\n",
    "test_modelB = model_names[1]\n",
    "case_dataset = get_dataset(\n",
    "        infer_model=test_modelA,\n",
    "        ref_model=test_modelB,\n",
    "        question_dataframe=quesion_dfs,\n",
    "        QA_dataframe=QA_df,\n",
    "        domains=DOMAIN\n",
    "    )\n",
    "evaluation_output = case_dataset.evaluate([correctness_metric])\n",
    "evaluation_output = evaluate(case_dataset, \n",
    "                             [correctness_metric], \n",
    "                             display_config=display_config,\n",
    "                            #  hyperparameters={\"Temperature\": 0.1, \"Max Tokens\": 50000, \"System Prompt\": \"You MUST NOT add any extra commentary outside the JSON\"}\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_matrix = np.zeros((len(alter_names), len(alter_names), len(DOMAIN) * num_questions_per_domain))\n",
    "print(correctness_matrix.shape)\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i+1, len(model_names)):\n",
    "        inference_model = model_names[i]\n",
    "        reference_model = model_names[j]\n",
    "        case_dataset = get_dataset(\n",
    "                infer_model=inference_model,\n",
    "                ref_model=reference_model,\n",
    "                question_dataframe=quesion_dfs,\n",
    "                QA_dataframe=QA_df,\n",
    "                domains=DOMAIN\n",
    "            )\n",
    "        print(f\"Evaluating {i}_{inference_model} vs {j}_{reference_model}\")\n",
    "        evaluation_output = evaluate(\n",
    "            case_dataset, \n",
    "            [correctness_metric], \n",
    "            display_config=display_config,\n",
    "            hyperparameters={\"Temperature\": 0.1, \"Max Tokens\": 50000,}\n",
    "                )\n",
    "        scores = np.array([evaluation_output.test_results[k].metrics_data[0].score for k in range(len(evaluation_output.test_results))])\n",
    "        correctness_matrix[i, j, :] = scores\n",
    "\n",
    "        inference_model_name = alter_names[j]\n",
    "        reference_model_name = alter_names[i]\n",
    "        case_dataset = get_dataset(\n",
    "                infer_model=inference_model,\n",
    "                ref_model=reference_model,\n",
    "                question_dataframe=quesion_dfs,\n",
    "                QA_dataframe=QA_df,\n",
    "                domains=DOMAIN\n",
    "            )\n",
    "        print(f\"Evaluating {j}_{inference_model_name} vs {i}_{reference_model_name}\")\n",
    "        evaluation_output = evaluate(\n",
    "            case_dataset,\n",
    "            [correctness_metric], \n",
    "            display_config=display_config,\n",
    "            hyperparameters={\"Temperature\": 0.1, \"Max Tokens\": 50000,}\n",
    "            )\n",
    "        scores = np.array([evaluation_output.test_results[k].metrics_data[0].score for k in range(len(evaluation_output.test_results))])\n",
    "        correctness_matrix[j, i, :] = scores\n",
    "np.save('deepeval_correctness_matrix.npy', correctness_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
