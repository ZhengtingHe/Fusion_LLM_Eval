{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f971f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1-Distill-Qwen-32B\n",
      "DeepSeek-R1-Distill-Qwen-14B\n",
      "deepseek/deepseek-r1\n",
      "openai/gpt-4.1\n",
      "openai/o3\n",
      "anthropic/claude-sonnet-4\n",
      "google/gemini-2.5-pro-preview\n",
      "x-ai/grok-3-beta\n",
      "共有13个领域，每个领域有10个问题\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from deepeval import evaluate\n",
    "from deepeval.evaluate import DisplayConfig\n",
    "display_config = DisplayConfig(\n",
    "    show_indicator=True,\n",
    "    print_results=False,\n",
    "    verbose_mode=False,\n",
    ")\n",
    "import toml\n",
    "with open('config.toml', 'r', encoding='utf-8') as toml_file:\n",
    "    config = toml.load(toml_file)\n",
    "\n",
    "model_names = config['model_names']\n",
    "alter_names = model_names.copy() # Create a copy to modify\n",
    "\n",
    "for i, name in enumerate(alter_names):\n",
    "    if name in config['alternative_names']: \n",
    "        alter_names[i] = config['alternative_names'][name]\n",
    "\n",
    "for name in alter_names:\n",
    "    print(name)\n",
    "INPUT_EXCEL_FILE = \"goldens\" / Path(config['QA_file_name'])\n",
    "quesion_dfs = pd.read_excel(INPUT_EXCEL_FILE, sheet_name=None, index_col=0)\n",
    "DOMAIN = list(quesion_dfs.keys())\n",
    "num_questions_per_domain = quesion_dfs[DOMAIN[0]].shape[0]\n",
    "print(f\"共有{len(DOMAIN)}个领域，每个领域有{num_questions_per_domain}个问题\")\n",
    "\n",
    "QA_df = {}\n",
    "for i, model in enumerate(model_names):\n",
    "    QA_FILE = \"QA\" / Path(f\"{model}_answers.xlsx\")\n",
    "    QA_df[model_names[i]] = pd.read_excel(QA_FILE, sheet_name=None, index_col=0)\n",
    "from custom_metrics import get_dataset, correctness_metric, relevance_metric\n",
    "from deepeval import evaluate\n",
    "from deepeval.dataset import EvaluationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ab34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 07:53,  3.65s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_modelA = model_names[1]\n",
    "test_modelB = model_names[5]\n",
    "case_dataset = get_dataset(\n",
    "        infer_model=test_modelA,\n",
    "        ref_model=test_modelB,\n",
    "        QA_dataframe=QA_df,\n",
    "        domains=DOMAIN\n",
    "    )\n",
    "evaluation_output = evaluate(case_dataset, \n",
    "                             [correctness_metric], \n",
    "                             display_config=display_config,\n",
    "                            #  hyperparameters={\"Temperature\": 0.1, \"Max Tokens\": 50000, \"System Prompt\": \"You MUST NOT add any extra commentary outside the JSON\"}\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8, 130)\n",
      "Evaluating 0_新奥QA测试-32B蒸馏 vs 1_新奥QA测试-14B蒸馏\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 07:26,  3.44s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 1_DeepSeek-R1-Distill-Qwen-14B vs 0_DeepSeek-R1-Distill-Qwen-32B\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 05:20,  2.47s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0_新奥QA测试-32B蒸馏 vs 2_deepseek/deepseek-r1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 06:24,  2.96s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 2_deepseek/deepseek-r1 vs 0_DeepSeek-R1-Distill-Qwen-32B\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 04:35,  2.12s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0_新奥QA测试-32B蒸馏 vs 3_openai/gpt-4.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 05:04,  2.34s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 3_openai/gpt-4.1 vs 0_DeepSeek-R1-Distill-Qwen-32B\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 05:54,  2.73s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0_新奥QA测试-32B蒸馏 vs 4_openai/o3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 06:41,  3.09s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 4_openai/o3 vs 0_DeepSeek-R1-Distill-Qwen-32B\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 04:56,  2.28s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0_新奥QA测试-32B蒸馏 vs 5_anthropic/claude-sonnet-4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 06:54,  3.19s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 5_anthropic/claude-sonnet-4 vs 0_DeepSeek-R1-Distill-Qwen-32B\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 05:48,  2.68s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 0_新奥QA测试-32B蒸馏 vs 6_google/gemini-2.5-pro-preview\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 05:57,  2.75s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 6_google/gemini-2.5-pro-preview vs 0_DeepSeek-R1-Distill-Qwen-32B\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">正确率 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r1 </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m正确率 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r1 \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |█████████▉| 99% (129/130) [Time Taken: 46:48, 21.77s/test case]\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 502",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m\n\u001b[1;32m     26\u001b[0m case_dataset \u001b[38;5;241m=\u001b[39m get_dataset(\n\u001b[1;32m     27\u001b[0m         infer_model\u001b[38;5;241m=\u001b[39minference_model,\n\u001b[1;32m     28\u001b[0m         ref_model\u001b[38;5;241m=\u001b[39mreference_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m         domains\u001b[38;5;241m=\u001b[39mDOMAIN\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minference_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m evaluation_output \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcase_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcorrectness_metric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# hyperparameters={\"Temperature\": 0.1, \"Max Tokens\": 50000,}\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([evaluation_output\u001b[38;5;241m.\u001b[39mtest_results[k]\u001b[38;5;241m.\u001b[39mmetrics_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(evaluation_output\u001b[38;5;241m.\u001b[39mtest_results))])\n\u001b[1;32m     41\u001b[0m correctness_matrix[j, i, :] \u001b[38;5;241m=\u001b[39m scores\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/deepeval/evaluate/evaluate.py:274\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(test_cases, metrics, hyperparameters, goldens, observed_callback, identifier, async_config, display_config, cache_config, error_config)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m async_config\u001b[38;5;241m.\u001b[39mrun_async:\n\u001b[1;32m    273\u001b[0m     loop \u001b[38;5;241m=\u001b[39m get_or_create_event_loop()\n\u001b[0;32m--> 274\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43ma_execute_test_cases\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43midentifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_to_disk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshow_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43mthrottle_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrottle_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_concurrent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_concurrent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m execute_test_cases(\n\u001b[1;32m    291\u001b[0m         test_cases,\n\u001b[1;32m    292\u001b[0m         metrics,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m         verbose_mode\u001b[38;5;241m=\u001b[39mdisplay_config\u001b[38;5;241m.\u001b[39mverbose_mode,\n\u001b[1;32m    300\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/asyncio/tasks.py:234\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    232\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/deepeval/evaluate/execute.py:501\u001b[0m, in \u001b[0;36ma_execute_test_cases\u001b[0;34m(test_cases, metrics, ignore_errors, skip_on_missing_params, use_cache, show_indicator, throttle_value, max_concurrent, save_to_disk, verbose_mode, identifier, test_run_manager, _use_bar_indicator, _is_assert_test)\u001b[0m\n\u001b[1;32m    498\u001b[0m                     tasks\u001b[38;5;241m.\u001b[39mappend(asyncio\u001b[38;5;241m.\u001b[39mcreate_task(task))\n\u001b[1;32m    500\u001b[0m                 \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(throttle_value)\n\u001b[0;32m--> 501\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m test_case \u001b[38;5;129;01min\u001b[39;00m test_cases:\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/asyncio/tasks.py:234\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    232\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/deepeval/evaluate/execute.py:394\u001b[0m, in \u001b[0;36ma_execute_test_cases.<locals>.execute_with_semaphore\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute_with_semaphore\u001b[39m(func: Callable, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/deepeval/evaluate/execute.py:614\u001b[0m, in \u001b[0;36ma_execute_llm_test_cases\u001b[0;34m(metrics, test_case, test_run_manager, test_results, count, test_run, ignore_errors, skip_on_missing_params, use_cache, show_indicator, _use_bar_indicator, _is_assert_test, pbar)\u001b[0m\n\u001b[1;32m    612\u001b[0m new_cached_test_case: CachedTestCase \u001b[38;5;241m=\u001b[39m CachedTestCase()\n\u001b[1;32m    613\u001b[0m test_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 614\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m measure_metrics_with_indicator(\n\u001b[1;32m    615\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mmetrics,\n\u001b[1;32m    616\u001b[0m     test_case\u001b[38;5;241m=\u001b[39mtest_case,\n\u001b[1;32m    617\u001b[0m     cached_test_case\u001b[38;5;241m=\u001b[39mcached_test_case,\n\u001b[1;32m    618\u001b[0m     skip_on_missing_params\u001b[38;5;241m=\u001b[39mskip_on_missing_params,\n\u001b[1;32m    619\u001b[0m     ignore_errors\u001b[38;5;241m=\u001b[39mignore_errors,\n\u001b[1;32m    620\u001b[0m     show_indicator\u001b[38;5;241m=\u001b[39mshow_metrics_indicator,\n\u001b[1;32m    621\u001b[0m )\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metric\u001b[38;5;241m.\u001b[39mskipped:\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/deepeval/metrics/indicator.py:222\u001b[0m, in \u001b[0;36mmeasure_metrics_with_indicator\u001b[0;34m(metrics, test_case, cached_test_case, ignore_errors, skip_on_missing_params, show_indicator, pbar_eval, _in_component)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m         tasks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    212\u001b[0m             safe_a_measure(\n\u001b[1;32m    213\u001b[0m                 metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m             )\n\u001b[1;32m    220\u001b[0m         )\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/deepeval/metrics/indicator.py:234\u001b[0m, in \u001b[0;36msafe_a_measure\u001b[0;34m(metric, tc, ignore_errors, skip_on_missing_params, pbar_eval, _in_component)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msafe_a_measure\u001b[39m(\n\u001b[1;32m    226\u001b[0m     metric: Union[BaseMetric, BaseMultimodalMetric, BaseConversationalMetric],\n\u001b[1;32m    227\u001b[0m     tc: Union[LLMTestCase, MLLMTestCase, ConversationalTestCase],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m     _in_component: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    232\u001b[0m ):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m metric\u001b[38;5;241m.\u001b[39ma_measure(\n\u001b[1;32m    235\u001b[0m             tc, _show_indicator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, _in_component\u001b[38;5;241m=\u001b[39m_in_component\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pbar_eval:\n\u001b[1;32m    238\u001b[0m             pbar_eval\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/deepeval/metrics/g_eval/g_eval.py:139\u001b[0m, in \u001b[0;36mGEval.a_measure\u001b[0;34m(self, test_case, _show_indicator, _in_component, _additional_context)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m metric_progress_indicator(\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    132\u001b[0m     async_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    133\u001b[0m     _show_indicator\u001b[38;5;241m=\u001b[39m_show_indicator,\n\u001b[1;32m    134\u001b[0m     _in_component\u001b[38;5;241m=\u001b[39m_in_component,\n\u001b[1;32m    135\u001b[0m ):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_steps: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a_generate_evaluation_steps()\n\u001b[1;32m    138\u001b[0m     )\n\u001b[0;32m--> 139\u001b[0m     g_score, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a_evaluate(\n\u001b[1;32m    140\u001b[0m         test_case, _additional_context\u001b[38;5;241m=\u001b[39m_additional_context\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;241m=\u001b[39m reason\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28mfloat\u001b[39m(g_score) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrict_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(g_score)\n\u001b[1;32m    145\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/deepeval/metrics/g_eval/g_eval.py:273\u001b[0m, in \u001b[0;36mGEval._a_evaluate\u001b[0;34m(self, test_case, _additional_context)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mscore, res\u001b[38;5;241m.\u001b[39mreason\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39ma_generate(prompt)\n\u001b[1;32m    274\u001b[0m     data \u001b[38;5;241m=\u001b[39m trimAndLoadJson(res, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreason\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Fusion_LLM_Eval/custom_metrics.py:132\u001b[0m, in \u001b[0;36mCustomOpenAI.a_generate\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21ma_generate\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mainvoke(prompt)\n\u001b[1;32m    133\u001b[0m     content \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    134\u001b[0m     repaired_content \u001b[38;5;241m=\u001b[39m repair_json_string(content)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:393\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 393\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    394\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    395\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    396\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    397\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    398\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    399\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    400\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:967\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    965\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    966\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    968\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    969\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    914\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    915\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    923\u001b[0m             ]\n\u001b[1;32m    924\u001b[0m         )\n\u001b[0;32m--> 925\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    926\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    927\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    929\u001b[0m ]\n\u001b[1;32m    930\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1093\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1093\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m   1094\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1095\u001b[0m     )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1137\u001b[0m, in \u001b[0;36mBaseChatOpenAI._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m payload\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1137\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_async_client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload\n\u001b[1;32m   1139\u001b[0m     )\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1141\u001b[0m     _handle_openai_bad_request(e)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/openai/resources/beta/chat/completions.py:437\u001b[0m, in \u001b[0;36mAsyncCompletions.parse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[1;32m    432\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    433\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[1;32m    434\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    439\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m    440\u001b[0m         {\n\u001b[1;32m    441\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    442\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    443\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    449\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    450\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    451\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    452\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m    453\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    454\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    455\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m    456\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    457\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m    458\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: _type_to_response_format(response_format),\n\u001b[1;32m    459\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    460\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    461\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m    462\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    467\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    468\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    469\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    470\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    471\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m    472\u001b[0m         },\n\u001b[1;32m    473\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    474\u001b[0m     ),\n\u001b[1;32m    475\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    476\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m    477\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[1;32m    478\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[1;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    480\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[1;32m    481\u001b[0m     ),\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;66;03m# in the `parser` function above\u001b[39;00m\n\u001b[1;32m    484\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast(Type[ParsedChatCompletion[ResponseFormatT]], ChatCompletion),\n\u001b[1;32m    485\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    486\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py:1748\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1743\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1744\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1745\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1746\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1747\u001b[0m     )\n\u001b[0;32m-> 1748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py:1555\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1552\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1554\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1555\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 502"
     ]
    }
   ],
   "source": [
    "correctness_matrix = np.zeros((len(alter_names), len(alter_names), len(DOMAIN) * num_questions_per_domain))\n",
    "print(correctness_matrix.shape)\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i+1, len(model_names)):\n",
    "        inference_model = model_names[i]\n",
    "        reference_model = model_names[j]\n",
    "        case_dataset = get_dataset(\n",
    "                infer_model=inference_model,\n",
    "                ref_model=reference_model,\n",
    "                question_dataframe=quesion_dfs,\n",
    "                QA_dataframe=QA_df,\n",
    "                domains=DOMAIN\n",
    "            )\n",
    "        print(f\"Evaluating {i}_{inference_model} vs {j}_{reference_model}\")\n",
    "        evaluation_output = evaluate(\n",
    "            case_dataset, \n",
    "            [correctness_metric], \n",
    "            display_config=display_config,\n",
    "            # hyperparameters={\"Temperature\": 0.1, \"Max Tokens\": 50000,}\n",
    "                )\n",
    "        scores = np.array([evaluation_output.test_results[k].metrics_data[0].score for k in range(len(evaluation_output.test_results))])\n",
    "        correctness_matrix[i, j, :] = scores\n",
    "\n",
    "        inference_model_name = alter_names[j]\n",
    "        reference_model_name = alter_names[i]\n",
    "        case_dataset = get_dataset(\n",
    "                infer_model=inference_model,\n",
    "                ref_model=reference_model,\n",
    "                QA_dataframe=QA_df,\n",
    "                domains=DOMAIN\n",
    "            )\n",
    "        print(f\"Evaluating {j}_{inference_model_name} vs {i}_{reference_model_name}\")\n",
    "        evaluation_output = evaluate(\n",
    "            case_dataset,\n",
    "            [correctness_metric], \n",
    "            display_config=display_config,\n",
    "            # hyperparameters={\"Temperature\": 0.1, \"Max Tokens\": 50000,}\n",
    "            )\n",
    "        scores = np.array([evaluation_output.test_results[k].metrics_data[0].score for k in range(len(evaluation_output.test_results))])\n",
    "        correctness_matrix[j, i, :] = scores\n",
    "model_name = config['model']\n",
    "np.save(f'deepeval_correctness_matrix(judged by {model_name}).npy', correctness_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7138ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">过程分 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using DeepSeek Chat </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m过程分 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing DeepSeek Chat \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">相关性 </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using DeepSeek Chat </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">JSON</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255m相关性 \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing DeepSeek Chat \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mJSON\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 130 test case(s) in parallel: |██████████|100% (130/130) [Time Taken: 02:23,  1.11s/test case]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval login'</span> to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span> to get &amp; share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run <span style=\"color: #008080; text-decoration-color: #008080\">'deepeval login'</span> in the CLI. \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[1;32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI.\n",
       " \n",
       "✨👀 Looking for a place for your LLM test data to live 🏡❤️ ? Use \u001b[38;2;106;0;255mConfident AI\u001b[0m to get & share testing reports, \n",
       "experiment with models/prompts, and catch regressions for your LLM system. Just run \u001b[36m'deepeval login'\u001b[0m in the CLI. \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from custom_metrics import derivation_metric, relevance_metric\n",
    "test_modelA = model_names[0]\n",
    "test_modelB = model_names[4]\n",
    "case_dataset = get_dataset(\n",
    "        infer_model=test_modelA,\n",
    "        ref_model=test_modelB,\n",
    "        QA_dataframe=QA_df,\n",
    "        domains=DOMAIN\n",
    "    )\n",
    "evaluation_output = evaluate(case_dataset,\n",
    "                             [derivation_metric, relevance_metric], \n",
    "                             display_config=display_config,\n",
    "                            #  hyperparameters={\"Temperature\": 0.1, \"Max Tokens\": 50000, \"System Prompt\": \"You MUST NOT add any extra commentary outside the JSON\"}\n",
    "                             )\n",
    "derevation_scores = np.array([evaluation_output.test_results[k].metrics_data[0].score for k in range(len(evaluation_output.test_results))])\n",
    "relevance_scores = np.array([evaluation_output.test_results[k].metrics_data[1].score for k in range(len(evaluation_output.test_results))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3fbb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(derevation_scores, alpha=0.7, edgecolor='black', label='Derivation Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "651ccdfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(relevance_scores, alpha=0.7, edgecolor='black', label='Relevance Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5f025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
